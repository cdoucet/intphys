#!/usr/bin/env python3
"""This script prepares data to be released on www.intphys.com.

The input data directory must contain {dev, test and train}
subdirectories with data generated by intphys-1.0. It creates the
following tar.gz archives:
  - dev.tar.gz contains all the dev/ folder,
  - test.tar.gz contains all the test/ folder, metadata excepted,
  - test_metadata.tar.gz contains metadata of the test/ folder,
  - train.{1, 2, 3, 4}.tar.gz contain the train/ folder split in 4
    archives of approx. 40G.

"""

import argparse
import json
import logging
import os
import pathlib
import progressbar
import random
import re
import shutil
import sys
import tarfile
import tempfile


logging.basicConfig(
    level=logging.DEBUG, format='%(message)s')
log = logging.getLogger()


def join_data_subdirs(data_dir, output_dir):
    subdirs = [os.path.join(data_dir, d)
               for d in sorted(os.listdir(data_dir))]
    assert all(re.match('^[0-9]+$', os.path.basename(d)) for d in subdirs)

    log.info('preparing data from %s', data_dir)

    # merge the config.json files
    configs = [json.loads(open(os.path.join(d, 'config.json'), 'r').read())
               for d in subdirs]
    try:  # dev and test
        merged = {k: {kk: {kkk: sum(c[k][kk][kkk] for c in configs)
                           for kkk in configs[0][k][kk].keys()}
                      for kk in configs[0][k].keys()}
                  for k in configs[0].keys()}
    except AttributeError:  # train
        merged = {k: {kk: sum(c[k][kk] for c in configs)
                      for kk in configs[0][k].keys()}
                  for k in configs[0].keys()}

    open(os.path.join(output_dir, 'config.json'), 'w').write(
        json.dumps(merged, sort_keys=True, indent=4))

    # the data subdir is either test or train
    s = 'test' if os.path.isdir(os.path.join(subdirs[0], 'test')) else 'train'
    subdirs = [os.path.join(d, s) for d in subdirs]
    src_data = sorted((os.path.join(d, dd)) for d in subdirs for dd in os.listdir(d))
    log.info('found %s blocks in %s subfolders', len(src_data), len(configs))

    # adapt the index of src_data dirs
    try:  # dev and test
        block_total = {
            k + '_' + kk + '_' + kkk: v
            for k in merged.keys()
            for kk in merged[k].keys()
            for kkk, v in merged[k][kk].items()}
    except AttributeError:  # train
        block_total = {
            k + '_' + kk: v
            for k in merged.keys()
            for kk, v in merged[k].items()}

    # index offset for archived blocks (so that we have continuous
    # numbering across block types)
    offset = {}
    count = 0
    for k in sorted(block_total.keys()):
        offset[k] = count
        count += block_total[k]
    del count

    counter = {k: 0 for k in block_total.keys()}
    index_length = len(str(len(src_data)))
    def get_index(block_type):
        counter[block_type] += 1
        n = offset[block_type] + counter[block_type]
        return '0' * (index_length - len(str(n))) + str(n)

    # store symlinks from data_dir to output_dir
    links = []
    for src_dir in src_data:
        block_type = '_'.join(src_dir.split('/')[-1].split('_')[1:])

        links.append((src_dir, os.path.join(
            output_dir, get_index(block_type)+ '_' + block_type)))

    for src, dst in links:
        # check the content of the folder
        if s == 'test':
            assert (sorted(os.listdir(src)) ==
                    ['1', '2', '3', '4', 'params.json']), \
                    'bad content {}: {}'.format(src, os.listdir(src))
        else:  # train
            assert (sorted(os.listdir(src)) ==
                    ['depth', 'mask', 'params.json', 'scene', 'status.json'])

        # finally create the symlink
        os.symlink(src, dst)


def create_archive(archive, files, arcfiles):
    log.info('creating %s', archive)
    pbar = progressbar.ProgressBar(maxval=len(files)).start()
    tar = tarfile.open(archive, 'w:gz', dereference=True)
    for n, f in enumerate(files):
        tar.add(f, arcname=arcfiles[n])
        pbar.update(n+1)
    tar.close()
    pbar.finish()


def prepare_dev(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'dev')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # write the tar.gz from the tmp_dir
        create_archive(
            os.path.join(output_dir, 'dev.tar.gz'),
            [os.path.join(tmp_dir, c) for c in os.listdir(tmp_dir)],
            [os.path.join('dev', c) for c in os.listdir(tmp_dir)])


def prepare_train(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'train')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # write the tar.gz from the tmp_dir. Split in 4 subarchives of
        # equal lenght.
        l = [l for l in os.listdir(tmp_dir) if l != 'config.json']
        assert len(l) % 4 == 0
        n = len(l)//4
        chunks = [l[i:i+n] for i in range(0, len(l), n)]
        chunks[0].append('config.json')
        assert(sum(len(c) for c in chunks) == len(os.listdir(tmp_dir)))

        for n in range(4):
            create_archive(
                os.path.join(output_dir, 'train.' + str(n+1) + '.tar.gz'),
                [os.path.join(tmp_dir, c) for c in chunks[n]],
                [os.path.join('train', c) for c in chunks[n]])


def prepare_test(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'test')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # replace [1, 2, 3, 4] (where 1, 2 are possible and 3, 4 are
        # impossible) by a random permutation
        log.info('shuffling possible/impossible subblocks')
        tmp_dir2 = permute_subblocks(tmp_dir)

        log.info('retrieving metadata')
        meta_files = [str(p) for p in pathlib.Path(tmp_dir2).glob('**/*.json')]
        meta_arcfiles = [p.replace(tmp_dir2, 'test') for p in meta_files]
        create_archive(
            os.path.join(output_dir, 'test_metadata.tar.gz'),
            meta_files, meta_arcfiles)

        log.info('retrieving data')
        data_files = [str(p) for p in pathlib.Path(tmp_dir2).glob('**/*.png')]
        data_arcfiles = [p.replace(tmp_dir2, 'test') for p in data_files]
        create_archive(
            os.path.join(output_dir, 'test.tar.gz'),
            data_files, data_arcfiles)


def permute_subblocks(data_dir):
    # since subdirs of data_dir are symlinks to read-only data we
    # cannot modify them in place, working in a tmp dir instead
    tmp_dir = os.path.join(data_dir, 'shuffled')
    os.makedirs(tmp_dir)
    os.symlink(
        os.path.join(data_dir, 'config.json'),
        os.path.join(tmp_dir, 'config.json'))

    # a block_dir contains 1, 2, 3, 4 and params.json
    block_dirs = [
        d for d in os.listdir(data_dir)
        if not d.endswith('json') and not d == 'shuffled']

    for block_dir in block_dirs:
        # compute a random permutation
        order = [1, 2, 3, 4]
        random.shuffle(order)

        # prepare the destination directory
        os.makedirs(os.path.join(tmp_dir, block_dir))
        os.symlink(
            os.path.join(data_dir, block_dir, 'params.json'),
            os.path.join(tmp_dir, block_dir, 'params.json'))

        # move from original to permuted
        for i in range(4):
            src = os.path.join(data_dir, block_dir, str(i+1))
            dst = os.path.join(tmp_dir, block_dir, str(order[i]))
            os.symlink(src, dst)

    return tmp_dir


def parse_args():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument(
        'data_dir', help='the input data directory')
    parser.add_argument(
        '-o', '--output-dir',
        help=('the output directory where to write the tar.gz files, '
        'use data_dir by default'))
    args = parser.parse_args()
    return args.data_dir, args.output_dir if args.output_dir else args.data_dir


def main():
    data_dir, output_dir = parse_args()

    if not os.path.isdir(data_dir):
        raise IOError('"{}" is not a directory'.format(data_dir))
    data_dir = os.path.abspath(data_dir)

    if not os.path.isdir(output_dir):
        raise IOError('"{}" is not a directory'.format(output_dir))
    output_dir = os.path.abspath(output_dir)

    # prepare_dev(data_dir, output_dir)
    prepare_test(data_dir, output_dir)
    # prepare_train(data_dir, output_dir)


if __name__ == '__main__':
   main()

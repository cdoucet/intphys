#!/usr/bin/env python3
"""This script prepares data to be released on www.intphys.com.

The input data directory must contain {dev, test and train}
subdirectories with data generated by intphys-1.0. It creates the
following tar.gz archives:
  - dev.tar.gz contains all the dev/ folder,
  - test.tar.gz contains all the test/ folder, metadata excepted,
  - test_metadata.tar.gz contains metadata of the test/ folder,
  - train.{1, 2, 3, 4}.tar.gz contain the train/ folder split in 4
    archives of approx. 40G.

"""

import argparse
import json
import logging
import os
import pathlib
import progressbar
import random
import re
import tarfile
import tempfile


logging.basicConfig(
    level=logging.DEBUG, format='%(message)s')
log = logging.getLogger()


def join_data_subdirs(data_dir, output_dir):
    subdirs = [os.path.join(data_dir, d)
               for d in sorted(os.listdir(data_dir))]
    assert all(re.match('^[0-9]+$', os.path.basename(d)) for d in subdirs)

    log.info('preparing data from %s', data_dir)

    # merge the config.json files
    configs = [json.loads(open(os.path.join(d, 'config.json'), 'r').read())
               for d in subdirs]
    try:  # dev and test
        merged = {k: {kk: {kkk: sum(c[k][kk][kkk] for c in configs)
                           for kkk in configs[0][k][kk].keys()}
                      for kk in configs[0][k].keys()}
                  for k in configs[0].keys()}
    except AttributeError:  # train
        merged = {k: {kk: sum(c[k][kk] for c in configs)
                      for kk in configs[0][k].keys()}
                  for k in configs[0].keys()}

    open(os.path.join(output_dir, 'config.json'), 'w').write(
        json.dumps(merged, sort_keys=True, indent=4))

    # the data subdir is either test or train
    s = 'test' if os.path.isdir(os.path.join(subdirs[0], 'test')) else 'train'
    subdirs = [os.path.join(d, s) for d in subdirs]
    src_data = sorted(
        (os.path.join(d, dd)) for d in subdirs for dd in os.listdir(d))
    log.info('found %s blocks in %s subfolders', len(src_data), len(configs))

    # adapt the index of src_data dirs
    try:  # dev and test
        block_total = {
            k + '_' + kk + '_' + kkk: v
            for k in merged.keys()
            for kk in merged[k].keys()
            for kkk, v in merged[k][kk].items()}
    except AttributeError:  # train
        block_total = {
            k + '_' + kk: v
            for k in merged.keys()
            for kk, v in merged[k].items()}

    # index offset for archived blocks (so that we have continuous
    # numbering across block types)
    offset = {}
    count = 0
    for k in sorted(block_total.keys()):
        offset[k] = count
        count += block_total[k]
    del count

    counter = {k: 0 for k in block_total.keys()}
    index_length = len(str(len(src_data)))

    def get_index(block_type):
        counter[block_type] += 1
        n = offset[block_type] + counter[block_type]
        return '0' * (index_length - len(str(n))) + str(n)

    # store symlinks from data_dir to output_dir
    links = []
    for src_dir in src_data:
        block_type = '_'.join(src_dir.split('/')[-1].split('_')[1:])

        links.append((src_dir, os.path.join(
            output_dir, get_index(block_type) + '_' + block_type)))

    for src, dst in links:
        # check the content of the folder
        if s == 'test':
            assert (sorted(os.listdir(src)) ==
                    ['1', '2', '3', '4', 'params.json']), \
                    'bad content {}: {}'.format(src, os.listdir(src))
        else:  # train
            assert (sorted(os.listdir(src)) ==
                    ['depth', 'mask', 'params.json', 'scene', 'status.json'])

        # finally create the symlink
        os.symlink(src, dst)


def create_archive(archive, files, arcfiles):
    log.info('creating %s', archive)
    pbar = progressbar.ProgressBar(maxval=len(files)).start()
    tar = tarfile.open(archive, 'w:gz', dereference=True)
    for n, f in enumerate(files):
        tar.add(f, arcname=arcfiles[n])
        pbar.update(n+1)
    tar.close()
    pbar.finish()


def prepare_dev(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'dev')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # write the tar.gz from the tmp_dir
        create_archive(
            os.path.join(output_dir, 'dev.tar.gz'),
            [os.path.join(tmp_dir, c) for c in os.listdir(tmp_dir)],
            [os.path.join('dev', c) for c in os.listdir(tmp_dir)])


def prepare_train(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'train')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # write the tar.gz from the tmp_dir. Split in 4 subarchives of
        # equal lenght.
        l = [l for l in os.listdir(tmp_dir) if l != 'config.json']
        assert len(l) % 4 == 0
        n = len(l)//4
        chunks = [l[i:i+n] for i in range(0, len(l), n)]
        chunks[0].append('config.json')
        assert(sum(len(c) for c in chunks) == len(os.listdir(tmp_dir)))

        for n in range(4):
            create_archive(
                os.path.join(output_dir, 'train.' + str(n+1) + '.tar.gz'),
                [os.path.join(tmp_dir, c) for c in chunks[n]],
                [os.path.join('train', c) for c in chunks[n]])


def prepare_test(data_dir, output_dir):
    data_dir = os.path.join(data_dir, 'test')
    assert os.path.isdir(data_dir)

    with tempfile.TemporaryDirectory() as tmp_dir:
        # join the 1 and 2 subdirs into a single one
        join_data_subdirs(data_dir, tmp_dir)

        # replace [1, 2, 3, 4] (where 1, 2 are possible and 3, 4 are
        # impossible) by a random permutation
        log.info('shuffling possible/impossible subblocks')
        tmp_dir2 = permute_subblocks(tmp_dir)

        log.info('retrieving metadata')
        meta_files = [str(p) for p in pathlib.Path(tmp_dir2).glob('**/*.json')]
        meta_arcfiles = [p.replace(tmp_dir2, 'test') for p in meta_files]
        create_archive(
            os.path.join(output_dir, 'test_metadata.tar.gz'),
            meta_files, meta_arcfiles)

        log.info('retrieving data')
        data_files = [str(p) for p in pathlib.Path(tmp_dir2).glob('**/*.png')]
        data_arcfiles = [p.replace(tmp_dir2, 'test') for p in data_files]
        create_archive(
            os.path.join(output_dir, 'test.tar.gz'),
            data_files, data_arcfiles)

        log.info('retrieving nobjects')
        folders = (os.path.join(data_dir, d) for d in os.listdir(data_dir))
        folders = (d for d in folders if os.path.isdir(d))
        data_nobjects = []
        for folder in sorted(folders):
            params = os.path.join(folder, 'params.json')
            nobjects = len(json.load(open(params, 'r'))['objects'])
            data_nobjects.append('{} {}'.format(
                os.path.basename(folder), nobjects))
        open(os.path.join(output_dir, 'test_nobjects.txt'), 'w').write(
            '\n'.join(data_nobjects))


def permute_subblocks(data_dir):
    # since subdirs of data_dir are symlinks to read-only data we
    # cannot modify them in place, working in a tmp dir instead
    tmp_dir = os.path.join(data_dir, 'shuffled')
    os.makedirs(tmp_dir)
    os.symlink(
        os.path.join(data_dir, 'config.json'),
        os.path.join(tmp_dir, 'config.json'))

    # a block_dir contains 1, 2, 3, 4 and params.json
    block_dirs = [
        d for d in os.listdir(data_dir)
        if not d.endswith('json') and not d == 'shuffled']

    for block_dir in block_dirs:
        # compute a random permutation
        order = [1, 2, 3, 4]
        random.shuffle(order)

        # prepare the destination directory
        os.makedirs(os.path.join(tmp_dir, block_dir))
        os.symlink(
            os.path.join(data_dir, block_dir, 'params.json'),
            os.path.join(tmp_dir, block_dir, 'params.json'))

        # move from original to permuted
        for i in range(4):
            src = os.path.join(data_dir, block_dir, str(i+1))
            dst = os.path.join(tmp_dir, block_dir, str(order[i]))
            os.symlink(src, dst)

    return tmp_dir


def parse_args():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument(
        'data_dir', help='the input data directory')
    parser.add_argument(
        '-o', '--output-dir',
        help=('the output directory where to write the tar.gz files, '
              'use data_dir by default'))
    args = parser.parse_args()
    return args.data_dir, args.output_dir if args.output_dir else args.data_dir


def main():
    data_dir, output_dir = parse_args()

    if not os.path.isdir(data_dir):
        raise IOError('"{}" is not a directory'.format(data_dir))
    data_dir = os.path.abspath(data_dir)

    if not os.path.isdir(output_dir):
        raise IOError('"{}" is not a directory'.format(output_dir))
    output_dir = os.path.abspath(output_dir)

    # prepare_dev(data_dir, output_dir)
    prepare_test(data_dir, output_dir)
    # prepare_train(data_dir, output_dir)


if __name__ == '__main__':
    main()
